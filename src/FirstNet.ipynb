{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCyU74xyoFJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip images.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMbEXcwMoZcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataloader import HolonomicDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF9gc8jrobIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "csv_file = 'data.csv'\n",
        "grayscale = True\n",
        "full_dataset = HolonomicDataset(csv_file,'',grayscale=grayscale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P28GGs5tofg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(full_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNgHzxbVor8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "\n",
        "class FirstNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FirstNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 8, 3, padding=1) \n",
        "    self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    #We need to introduce dropout for stochasticity\n",
        "    self.fc1 = nn.Linear(8 * 8 * 64 + 4, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 512)\n",
        "    self.fc3 = nn.Linear(512, 256)\n",
        "    self.fc4 = nn.Linear(256, 128)\n",
        "    self.fc5 = nn.Linear(128, 8)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x=input[1]\n",
        "    x=torch.unsqueeze(x,0).float()\n",
        "    x = F.max_pool2d(F.relu(self.conv1(x)), 2) # 64x64\n",
        "    x = F.max_pool2d(F.relu(self.conv2(x)), 2) # 32x32\n",
        "    x = F.max_pool2d(F.relu(self.conv3(x)), 2) # 16x16\n",
        "    x = F.max_pool2d(F.relu(self.conv4(x)), 2) # 8X8\n",
        "\n",
        "    x = x.view(-1, self.num_flat_features(x))\n",
        "    #X = torch.cat((torch.transpose(x,0,1), torch.transpose(input[0].float(),0,1), 0))\n",
        "    x=torch.squeeze(x)\n",
        "    input[0]=torch.squeeze(input[0])\n",
        "    X = torch.cat((x,input[0].float()),0)\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = F.relu(self.fc3(X))\n",
        "    X = F.relu(self.fc4(X))\n",
        "    X = torch.sigmoid(self.fc5(X))\n",
        "    return X\n",
        "\n",
        "  def num_flat_features(self, x):\n",
        "    size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "        num_features *= s\n",
        "    return num_features\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3NME77Rvbn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#instance of the Conv Net\n",
        "net = FirstNet().float()\n",
        "learning_rate=0.0001\n",
        "num_epochs=2\n",
        "batch_size=1\n",
        "#loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUm-e5Iywfny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b15508e6-cabe-4556-c4cf-d7e0977f7045"
      },
      "source": [
        "losses = [];\n",
        "# for epoch in range(num_epochs):\n",
        "for i, (input_data, labels) in enumerate(trainloader):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = torch.unsqueeze(net(input_data), 0)\n",
        "    labels -= 1 # shift labels from 1-8 to 0-7 (fix off by one)\n",
        "    loss = criterion(outputs, labels) # turns out one-hot encoding is not necessary\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if (i+1) % 100 == 0:\n",
        "        print ('Epoch : %d/%d, Iter : %d/%d,  Loss: %.4f' \n",
        "                %(epoch+1, num_epochs, i+1, len(full_dataset)//batch_size, loss.item()))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1/2, Iter : 100/10719,  Loss: 1.4368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlOJYqB9xhbF",
        "colab_type": "text"
      },
      "source": [
        "1. Softmax/ Sigmoid Activation at the final layer\n",
        "2. One hot encoding for the lebels"
      ]
    }
  ]
}